\documentclass[11pt]{article}
\usepackage{amsmath}
\usepackage{amsfonts} 
\usepackage[english]{babel}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{subcaption}
\usepackage{hyperref}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{anyfontsize} % fix font size warning. 
\usepackage{url} 
\urlstyle{same} % fix wacky url links in bib entries. 
% \usepackage{minted}
\usepackage[switch, displaymath, mathlines]{lineno}



% Basic Type Settings ----------------------------------------------------------
\usepackage[margin=1in,footskip=0.25in]{geometry}
\linespread{1}  % double spaced or single spaced
\usepackage[fontsize=12pt]{fontsize}
\usepackage{authblk}

\theoremstyle{definition}
\newtheorem{theorem}{Theorem}       % Theorem counter global 
\newtheorem{prop}{Proposition}[section]  % proposition counter is section
\newtheorem{claim}{Claim}[subsection]
\newtheorem{lemma}{Lemma}[subsection]  % lemma counter is subsection
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}[subsection]
{
    % \theoremstyle{plain}
    \newtheorem{assumption}{Assumption}
}
\numberwithin{equation}{subsection}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
}
\usepackage[final]{graphicx}
\usepackage{listings}
\usepackage{courier}
\lstset{basicstyle=\footnotesize\ttfamily,breaklines=true}
\newcommand{\indep}{\perp \!\!\! \perp}
\usepackage{wrapfig}
\graphicspath{{.}}
\usepackage{fancyvrb}

%%
%% Julia definition (c) 2014 Jubobs
%%
\usepackage[T1]{fontenc}
\usepackage{beramono}
\usepackage[usenames,dvipsnames]{xcolor}
\lstdefinelanguage{Julia}%
  {morekeywords={abstract,break,case,catch,const,continue,do, else, elseif,%
      end, export, false, for, function, immutable, import, importall, if, in,%
      macro, module, otherwise, quote, return, switch, true, try, type, typealias,%
      using, while},%
   sensitive=true,%
   alsoother={$},%
   morecomment=[l]\#,%
   morecomment=[n]{\#=}{=\#},%
   morestring=[s]{"}{"},%
   morestring=[m]{'}{'},%
}[keywords,comments,strings]%
\lstset{%
    language         = Julia,
    basicstyle       = \ttfamily,
    keywordstyle     = \bfseries\color{blue},
    stringstyle      = \color{magenta},
    commentstyle     = \color{ForestGreen},
    showstringspaces = false,
}

\title{Modeling and Algorithms for Prof Shi's Project}
\author{Hongda Li, Yining Zhou, Xiaoping Shi}

\begin{document}


\maketitle

\begin{abstract}
    We propose some better algorithm for a problem in detecting structure of probability transition matrices from data. 
\end{abstract}


\section{Introduction}
    We describe an optimization problem introduced by Prof Shi and his student Yining. 
    To start we define the following quantities for the optimization problem. 
    \begin{itemize}
        \item [1.] $n\in \mathbb N$. It denotes the numer of states for a Markov Chain. 
        \item [2.] $p\in \mathbb R^{n\times n}$ denotes the probability transition matrix. It's in small case because it's also the variable for the optimization problem. It supports two types of indexing, $p_{i,j}$ for $i, j \in \{1, \cdots, n\}$, or $p_j$ with $j\in \{1, \cdots, n^2\}$. More on this later. 
        \item [3.] $\eta_{ij} \ge 0$ for $i, j \in \{1, \cdots, n\}$ is a parameter of the problem. 
        \item [4.] $\hat p$ is the empirically measured probability transition matrix. They are the maximal likelihood estimators for the transition probability in the transition probability matrix. 
        \item [5.] $\lambda$ is the regularization parameter. 
        \item [6.] $\mathbf C^m_n$ is the combinatoric term that counts all possible subset of size $n, n<m$ from a super set of size $m$. 
    \end{itemize}
    \par
    When $p$ is referred to as a vector we may say $p \in \mathbb R^{n^2}$, if it's referred to as the matrix, we will use $p \in \mathbb R^{n\times n}$. 
    When indexing $p$ using a tuple, or a single number, it's possible to translate between the two type of indexing scheme using the following bijective map: 
    \begin{align*}
        & (i, j) \mapsto k:= i \times n + j
        \\
        & k \mapsto (i, j) := (
            \lfloor k/n\rfloor, \text{mod}(k, n) + 1
        ). 
    \end{align*}
    We emphasize, in different programming languages and development environments, the convention of indexing a muti-array using different kind of tuples can be very different. 
    For now we use the above indexing, which is a row major index convention (Like Python). 
    \subsection{Some Mathematical Entities}
        We introduce some mathematical entities for more context. 

    \subsection{The Optimization Problem}
    The optimization problem is posed as 
        \begin{align}
            &\underset{p \in \mathbb R^{n^2}}{\text{argmin}} 
            \left\lbrace
                \sum_{i = 1}^{n}\sum_{j = 1}^{n}
                    -\eta_{ij}\log(p_{i, j})
                + 
                \lambda \sum_{i=1}^{n^2}\sum_{\substack{j=1\\j\neq i}}^{n^2}
                \frac{1}{2}
                \frac{|p_{i} - p_j|}{|\hat p_{i} - \hat p_j|}
                \right\rbrace
                \\
                &\text{ s.t: }
                \left\lbrace
                \begin{aligned}
                    \sum_{i = 1}^{n} p_{i, j} &= 1 \; && \forall i =1, \cdots, n 
                    \\
                    p_{i, j} &\ge 0\; &&\forall i= 1,\cdots, n, j = 1,\cdots, n 
                \end{aligned}
                \right\rbrace
            \label{eqn:original_formulaion}
        \end{align}
        There are 3 parts to the optimization problem posed above. 
        It has a smooth differentiable function, the sume of $\eta_{ij}\log(p_{i,j})$ but its gradient is not Lipschitz. 
        The ias a non-smooth part with $p_i - p_j$ for all indices $1,\cdots, n^2$ and $i\neq j$. 
        If $\hat p_i - \hat p_j = 0$, then we would ignore the term. 
        Finally, it has a linear constraints on all $n^2$ variables. 
        $p\in \mathbb R^{n^2}$ is a vector with the structure $\Delta_n\oplus\Delta_n\oplus \cdots \oplus \Delta_n$. 
        Each $\Delta_n$ is a probability simplex. 
        It's defined as $\Delta_n = \{x\in \mathbb R^n_+ : \sum_{i =1}^{n}x_n = 1\}$. 
        For simplicity we just denote using notation $\Delta_{n\times n} = \Delta_n\oplus\Delta_n\oplus \cdots \oplus \Delta_n$. 

\section{Modeling}
    
    The non-smooth part with the absolute value requires some amount of creativity if we were to use common optimization algorithms. 
    \subsection{Representation of the Non-smooth Part}\label{sec:opsplit_model}
        \begin{claim}
            The nonsmooth objective can be model as 
            \begin{align*}
                \lambda \sum_{i=1}^{n^2}\sum_{\substack{j> i}}^{n^2}
                \frac{|p_{i} - p_j|}{|\hat p_{i} - \hat p_j|}
                &= 
                \left\Vert
                    Cp
                \right\Vert_1 \quad 
                \text{ where $C$ is $\mathbf C_2^{n^2}$ by $n^2$}. 
            \end{align*}
            The one norm represents the summation of absolute values. 
            The transformation $\mathbf Cp$ is linear transformation and it's a vector of length $\mathbf C_2^{n^2}$. 
            The vector is long and it has a dimension of $(1/2)(1 +n^2)n^2$. 
            Each term inside of the summation is a row of the matrix $C$. 
            Each row of matrix $C$ has exactly 2 non-zero elements in it. 
            Suppose that $i\in \{1,\cdots,\mathbf C^{n^2}_2\}$ denoting the index for a specific row of matrix $C$ denotes the index of a specific row, and $j\in \{1, \cdots, n^2\}$ denotes a specific column of matrix $C$. 
            Mathematically describing the matrix is difficult, but it can be algorithmically defined.
            A sparse matrix format can be described by as a mapping from $(i,j)$, the set of all indices to the element in the vector. 
            The following 
            \hyperref[alg:matrixmk]{algorithm \ref{alg:matrixmk}}
            construct such a mapping. 
            \begin{algorithm}[H]
                \begin{algorithmic}[1]
                \STATE{Let $C$ be a $\mathbf C_2^{n^2}$ by $n^2$ zero matrix. }
                \FOR{$i = 1,\cdots, n^2$}
                    \FOR{$j = 1,\cdots, n^2$}
                        \IF{$|\hat p_i - \hat p_j| == 0$} 
                            \STATE{
                                \textbf{continue}
                            }
                        \ENDIF
                        \STATE{
                            $C[i\times n^2 + j, i] := \lambda/|\hat p_{i} - \hat p_{j}|$
                            } 
                        \STATE{
                            $C[i\times n^2 + j, j]:= -\lambda/|\hat p_{i} - \hat p_{j}|$ 
                        }
                    \ENDFOR
                \ENDFOR
                \end{algorithmic}
                \caption{Matrix Make Algorithm}
                \label{alg:matrixmk}
            \end{algorithm}
            
        \end{claim}
        \begin{remark}
            In practice, we should use sparse matrix data format such as SCR, SCC in programming languages. 
        \end{remark}
    \subsection{Modeling it for Sequential Quadratic Programming}\label{sec:sqp_formulation}
        To start, refer to \href{https://en.wikipedia.org/wiki/Sequential_quadratic_programming}{Seqential Qaudratic Programming Wikipedia} for brief overview about what sequential quadratic programming problem is. 
        To use sequential programming, we model the non-smooth $\Vert Cp\Vert_1$ parts of the objective function as a linear constraints. 
        Define $u\in \mathbb R^{\mathbf C^{n^2}_2}_+$ then the below problem is equivalent to the original formulation:
        \begin{align}
            &\underset{
                \substack{
                        p, u
                    }
            }{\text{argmin}} 
            \left\lbrace
                \sum_{i = 1}^{n}\sum_{j = 1}^{n}
                    -\eta_{ij}\log(p_{i, j})
                + 
                \sum_{i = 1}^{\mathbf C_2^{n^2}} u_k
            \right\rbrace
            \\
            &\text{ s.t: }
            \left\lbrace
            \begin{aligned}
                - u \le Cp \le u
                \\
                p \in \Delta_{n\times n}
                \\ 
                u \in \mathbb R^{\mathbf C^{n^2}_2}_+
            \end{aligned}
            \right\rbrace
        \end{align}
        This is a Non-linear programming problem and it has a convex objective. 
        Common NLP packages in programming languages can solve this efficiently. 
        However it's potentially possible that these solvers are not adapated to huge sparse matrix $C$ that has special structure to it. 
        \begin{remark}
            To formulate into a linear programming with some relaxations, consider that the non-linear objective $-\eta_{ij}\log(p_{i,j})$ can be discretized. 
        \end{remark}
    \subsection{Modeling it for Operator Splitting}
        Operator splitting method aims for objective function of the type $f + g$ where $f, g$ are both convex function and they are proximal friendly. 
        And $\text{ri.dom}(f)\cap \text{ri.dom}(g) \neq \emptyset$. 
        Recall that proximal operator of function $f$ is the resolvent operator on the subgradient $(I + \partial f)^{-1}$. 
        Subgradient is just a generalize type of gradient that can handle continuous function that is not necessarily differentiable. 
        To model the original formulation in such a form, we introduce these quantities: 
        \begin{itemize}
            \item [1.] $f_1(x_1): \mathbb R^{n^2}\mapsto \mathbb R := \sum_{i = 1}^{n^2}-\eta_{i, j}(\log(p_{i, j}))$
            \item [2.] $f_2(x_2): \mathbb R^{\mathbf C^{n^2}_2}\mapsto \mathbb R := \Vert x_2\Vert_1$. 
            \item [3.] $f_3(x_3): \mathbb R^{n^2}\mapsto \mathbb {\bar R}:= \delta_{\Delta_{n\times n}}(x_3)$
        \end{itemize}
        The above three functions represent the three parts of the summed objective. 
        \\
        Let $f(x_1, x_2, x_3):= f_1(x_1) + f_2(x_2) + f_3(x_3)$. 
        However, they share different variables $x_1, x_2, x_3$, from different dimension. 
        We want $g$ to represents the constraints that $x_2 = Cp$, and $x_1 = x_3 = p$. 
        Simplifying: $x_2 = C x_1, x_1 = x_3$. 
        This is a linear system of the form    
        \begin{align*}
            \begin{bmatrix}
                C & -I & \mathbf 0
                \\
                I & \mathbf 0 & -I
            \end{bmatrix}
            \begin{bmatrix}
                x_1 \\ x_2 \\ x_3
            \end{bmatrix}
            &= \mathbf 0. 
        \end{align*}
        Simply denote the above as $A x = \mathbf 0$, then $g$ is just a non-smooth function that happens to be an indicator function of a convex set. 
        The convex set is the set of all solutions to the linear system. 
        Therefore setting $g = \delta_{Ax = \mathbf 0}(x)$, to be the indicator of the linear constraints, we had a complete equivalent representation of the original form of the problem. 
        \par
        The matrix $A$ is a $(\mathbf C^{n^2}_2 + n^2) \times (3n^2)$ matrix. 
        It's still a sparse matrix. 
        The matrix would have to be encoded carefully by translating copies of the same matrix to 3 different locations. 
    \subsection{Another Method that is not Cursed by the Dimension}
        ...
    \subsection{A Quick Tour into Operator Splitting Method}
        Operator splitting method is relatively new and doesn't have a mature toolings in software. 
        Using the method requires the use of the Proximal Mapping, which can be non-trivial to implement for even simple functions. 
        Read section \cite[section 2.7]{ryu_large-scale_2022} of Ryu's Textbook for more information about the algorithm and operator splitting in general. 
        
        
\section{Implementation, Software, and Toolings}
    The software for Squential Quadratic programming is relatively mature. 
    Whether they support sparse matrix inversion depends on the developers. 
    The software operator splitting method is less common.
    To use Operator splitting method such as Douglas Rachford (Equivalently the ADMM via a the Dual), Peachman Rachford, it's required to compute the proximal operator of functions. 
    The following theorem is relevant to efficiently compute the proximal mapping for our model. 
    \begin{theorem}[Separable Proximal Theorem]
        Suppose that function $f(x)$ can be written as the Euclidean cross product $(f_1(x_1), f_2(x_2), \cdots, f_m(x_m))$, and we assume that each of $x_1, x_2, \cdots, x_n$ are vector in $\mathbb R^{k_1}, \mathbb R^{k_2}, \cdots, \mathbb R^{k_m}$ such that $x =\bigoplus{i=1}^m x_i$,then we can write the proximal gradient operator of $f$ in parallel form which is given as: 
        $$
        \begin{aligned}
            \text{prox}_{f, \lambda}((x_1, x_2, \cdots, x_m)) = 
            (\text{prox}_{f_1, \lambda}(x_1), \text{prox}_{f_2, \lambda}(x_2), \cdots \text{prox}_{f_m, \lambda}(x_m)).
        \end{aligned}
        $$
        In general, if there exists any permutation of the list of $x_1, x_2, \cdots, x_m$, the parallelization property of prox will be come applicable.     
    \end{theorem}
    Recall objective $f(x_1, x_2, x_3) = f_1(x_1) + f_2(x_2) + f_3(x_3)$ from section \ref*{sec:opsplit_model}. It has the applicable form and hence we have:     
    \[
        \underset{\lambda f}{\text{prox}}((x_1, x_2, x_3))
        = 
        \underset{\lambda f_1}{\text{prox}}(x_1) \oplus
        \underset{\lambda f_2}{\text{prox}}(x_2) \oplus
        \underset{\lambda f_3}{\text{prox}}(x_3). 
    \]
    Next, we proceed to write each $f_i$ for $i = 1, 2, 3$ into separable sum as well. 
    $f_1, f_2$ are trivial to do however $\delta_{\Delta_{n\times n}}$ requires some thought. 
    Consider $x_3 = \bigoplus_{i = 1}^n x_{3, i}$, so that the vector is the direct product, then     
    \[
        \delta_{\Delta_{n\times n}}(x_3) = 
        \sum_{i = 1}^{n} \delta_{\Delta_n}(x_{3, i}). 
    \]
    The indictor function $\delta_{\Delta_{n\times n}}$ acts on groups of $n$ elements in the vector $x_3$ individually and separately, hence it admits the above separable summation form. 
    We now list the proximal mapping for each of the atomic operatons: $\lambda\log(\cdot), \lambda|\cdot|, \delta_{\Delta_n}$ and $\delta_{Ax = \mathbf 0}(\cdot)$. 
    Most of then can be found in the literatures.     
    \begin{align*}
        \underset{-\lambda \log(\cdot)}{\text{prox}}(x)
        &= 
        \frac{x + \sqrt{x^2 + 4\lambda}}{2}
        \\
        \underset{\lambda |\cdot|}{\text{prox}}(x)
        &= 
        \text{sign}(x) \max(|x| - \lambda, 0)
        \\
        \underset{\delta_{\Delta_n}}{\text{prox}}&= 
        \Pi_{\Delta_{n}}
        \\
        \underset{\delta_{\{Ax = \mathbf 0\}}}{\text{prox}} &= 
        \Pi_{Ax = \mathbf 0}. 
    \end{align*}
    \subsection{Projection onto Probability Simplex and Sparse Linear System}
        Projecting onto a probability simplex and a linar system that is sparse requires a bit more attention for software implementation sake. 
        We introduce the following theorem from Beck textbook to assist with the projection onto a probability simplex. 
        \begin{theorem}[Projecting onto Hyperplane Box Intersections]\label{thm:proj_master}
            Let $C\subseteq \mathbb R^n$ to be non-empty and defined as $C := \text{Box}[l, u] \cap H_{a, b}^=$, 
            where $\text{Box}[l, u] = \{x\in \mathbb R^n : l \le x \le u\}$. We assume that $l\in [-\infty, \infty)$ and $u\in (-\infty, \infty]$. 
            And $H_{a, b}^= = \{x\in \mathbb R^n: \langle a, x\rangle = b\}$, where $a \in \mathbb R^n \setminus \{\mathbf 0\}$. 
            Then the projection onto the set is equivalent to the following conditions 
            \begin{align*}
                &\Pi_C(x) = \Pi_{\text{box}[l, u]}(x - \mu a)
                \\
                &\text{where }u \in \mathbb R \text{ solves: } \langle a, \Pi_{\text{Box}[l, u]}(x - u a)\rangle = b. 
            \end{align*}
        \end{theorem}
        \begin{proof}
            See \cite[thereom 6.27]{beck_first-order_nodate} by Beck for more information. 
        \end{proof}
        \begin{remark}
            The equation for $u$ has a continuous mapping $\mathbb R \mapsto \mathbb R$, it can be solved efficiently using method such as the bisection method. 
        \end{remark}
        \begin{theorem}[Projecting onto the Probability Simplex]\label{thm:proj_prob_simplex}
            The projection onto the simplex $\Delta_n$ can be computed via 
            \begin{align*}
                & \Pi_{\Delta_n}(x) = \Pi_{\mathbb R_+}(x - \mu a)
                \\
                & \text{where }\mu \in \mathbb R \text{ solves: } \langle \mathbf 1, \Pi_{\mathbb R_+}(x - \mu \mathbf 1)\rangle = 1.     
            \end{align*}
        \end{theorem}
        \begin{proof}
            Use the previous theorem \ref*{thm:proj_master}, observe that $u$ can be a vector of infinity and hence setting $\mathbb R_+$ to be the box, and the probability simplex consrants $\sum_{i = 1}^{n}x_i = 1$ is the hyperplane $H_{a, b}^=$ with $a = \mathbf 1, b = 1$. 
        \end{proof}
        \begin{theorem}[Projection onto Affine Spaces]
            Define the set $C = \{x\in \mathbb R^n : Ax = b\}$, assuming that $A\in \mathbb R^{m\times n}$ is full rank, then the projection onto the set is compted via 
            \begin{align*}
                \Pi_{C}(x) = x - (A^TA)^{-1}A(Ax - b). 
            \end{align*}
        \end{theorem}
        \begin{remark}
            In the case when $A$ is not full rank, do a reduce QR decomposition of $A = Q R$ where $R$ is $k\times n$ with $k < m$. 
            In the case of our application, the matrix $A$ is a full rank matrix. 
            The matrix $(A^TA)^{-1}$ in our case can be very slow to invert since it's $\mathbf C^{n^2}_2 \times \mathbf C^{n^2}_2$. 
            One way to make it easier is to use sparse matrix invert method. 
            In our case the matrix we are inverting is sparse and symmetric, it can be solved efficiently say for example, 
        \end{remark}
\section{The Software APIs for the Programmers}
    Option \href{https://docs.scipy.org/doc/scipy/reference/optimize.minimize-slsqp.html}{SLSQP} in ``scipy.optimize.minimize'' performs sequential quadratic programming. 
    I am not sure whether they would support efficient sparse matrix inverse. 
    \href{https://en.wikipedia.org/wiki/Interior-point_method}{Interior points method} (the engine behind many constraint optimization solver) is also a good way of solving the problem, it's also more specialized than the SLSQP solver. 
    However their availability in python seems to be limited.
    See \href{https://docs.scipy.org/doc/scipy/tutorial/optimize.html}{here} for tutorials about how to use ``scipy.optimize'' module. 
    Visits \href{https://pyproximal.readthedocs.io/en/stable/tutorials/index.html}{pyprox} for a package that implements various type of proximal operators in python.
    \subsection{Scipy.optimize.SLSQP}
        \href{https://docs.scipy.org/doc/scipy/tutorial/optimize.html}{scipy optimization documentations with examples.}
        The ``scipy.SLSQP" programming API accepts programming problem of the following form 
        \begin{align*}
            & \min_{l \le x\le u} 
            f(x)
            \text{ s.t: }
            \\
            & 
            \begin{cases}
                c_j(x) = 0, & j \in \Upsilon
                \\
                g_j(x) \ge 0, & j \in \mathcal I    
            \end{cases}
        \end{align*}
        For each element $i=n,\cdots, N$, we have the constraints in the form of $l_i \in [-\infty, \infty)$ and $u_i \in (-\infty, \infty]$. 
        The programmer is required to provide 
        \begin{itemize}
            \item[1.] $\nabla f(x)$, the gradient of the objective and the objective function in the form of a function handle in python. 
            \item[2.] 
        \end{itemize}
        
        Representing the sequence of constraints $c_j(x)$ for $J \in \Upsilon$ as a vector function $C(x)$. 
        Similarly, we represent inequality constraints as the vector funtion $G(x)$. 
        Recall formulation from \hyperref[sec:sqp_formulation]{section \ref*{sec:sqp_formulation}}. 
        The inequality constraints are $-u \le Cp \le u$, $p \in \Delta_{n\times n}$. 
        The box constraint is $u\in \mathbb R_+^{\mathbf C_2^{n^2}}$. 
        The equality and inequality constriants can be modeled by the linear system: 
        \begin{align*}
            \underbrace{\begin{bmatrix}
                -C & I
                \\
                C & I
            \end{bmatrix}}_{=: \mathcal C}
            \begin{bmatrix}
                p \\ u
            \end{bmatrix}
            \ge
            \mathbf 0 
            \\
            \underbrace{
                \begin{bmatrix}
                    \mathbf 1_n^T  & & &
                    \\
                    & \mathbf 1_n^T & &
                    \\
                    & & \ddots & 
                    \\
                    & & & \mathbf 1_n^T
                \end{bmatrix}
            }_{\mathcal G}
            p - \mathbf 1 &= \mathbf 0 .
        \end{align*}
        Here $\mathbf 1_n \in \mathbb R^n$ is a vector of $1$. 
        Observe that, $\mathcal C$ is a $2 \mathbf C_2^{n^2}$ by $2n + \mathbf C_2^{n^2}$ matrix. 
        $\mathcal G$ is a $n\times n^2$ matrix. 
        The matrix $\mathcal C, \mathcal G$ denoted above are the Jacobi of the inequality and equality constraints respectively. 
        Finally, the gradient of the objective function would be a vector with dimension $n^2 + \mathbf C_2^{n^2}$. 
        \newcommand{\elew}[1]{\;\textbf{#1}}
        Denote $\div$, to indicate that this is an operator that does element-wise division.
        For example, in below, we have $-\eta\div p$ , which means dividing each element of the vector $\mu$ by element of the vector $p$, resulting in an element of the same dimension. 
        \begin{align}
            \nabla \left[
                \sum_{i = 1}^{n^2} - \eta_{i} \log(p_i)
                + 
                \sum_{i = 1}^{\mathbf C_2^{n^2}} u_k
            \right]&= 
            \begin{bmatrix}
                -\eta \div p
                \\
                \mathbf 1_{\mathbf C_2^{n^2}}
            \end{bmatrix}. 
        \end{align}
        At this point, we have everything ready for coding it up in python. 
        





        


\appendix

\bibliographystyle{IEEEtran}
\bibliography{refs.bib}


\end{document}